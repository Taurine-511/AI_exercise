{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00391ca5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from IPython.display import FileLink\n",
    "\n",
    "EVAL_INSTRUCTIONS_COUNT = \"\"\"\n",
    "You are an evaluator. Your ONLY task is to output a single integer.\n",
    "\n",
    "Evaluation Rule:\n",
    "1. Count how many unique facts from the [Reference] list are present in the [Candidate] list.\n",
    "2. The score MUST NOT exceed the total number of items in the [Reference] list.\n",
    "3. If multiple sentences in the [Candidate] describe the same single fact in the [Reference], count it as only 1 match.\n",
    "4. Each item in the [Reference] can be marked as \"covered\" only once.\n",
    "5. Be generous with paraphrasing, but strict about the total count.\n",
    "\n",
    "Constraint (Strict):\n",
    "1. The VERY FIRST character of your response must be a digit (0-9).\n",
    "2. Do not include any preamble, thoughts, explanations, or \"The number is:\".\n",
    "3. Do not use any markdown, code blocks, or punctuation.\n",
    "\n",
    "[Reference]\n",
    "{reference}\n",
    "\n",
    "[Candidate]\n",
    "{candidate}\n",
    "\n",
    "Final Instruction: Output the integer now.\n",
    "\"\"\".strip()\n",
    "\n",
    "# ここだけ変更\n",
    "BASE_URL = \"〇〇\"\n",
    "API_KEY = \"〇〇\"\n",
    "INFER_JSON_PATH = \"inference_results_350samples.json\"\n",
    "\n",
    "\n",
    "JUDGE_MODEL = \"gpt-5-nano\"\n",
    "MODE = \"count\"\n",
    "MAX_COMPLETION_TOKENS = 4096\n",
    "\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "dataset = load_dataset(\"Lucas-Y04/clevr-change-subset_1\")\n",
    "\n",
    "def format_list(items):\n",
    "    return \"\\n\".join(f\"- {x}\" for x in items)\n",
    "\n",
    "def llm_judge(model_output: list[str], correct_answers: list[str], mode: str = \"count\", model: str = \"gpt-5-nano\"):\n",
    "    assert mode in {\"count\"}\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Reference (required facts):\n",
    "{format_list(correct_answers)}\n",
    "\n",
    "Candidate (model output):\n",
    "{format_list(model_output)}\n",
    "\n",
    "Please follow the evaluation task described above for the selected mode.\n",
    "\"\"\".strip()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": EVAL_INSTRUCTIONS_COUNT},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        max_completion_tokens=MAX_COMPLETION_TOKENS,\n",
    "    )\n",
    "\n",
    "    raw = (response.choices[0].message.content or \"\").strip()\n",
    "    match = re.search(r'\\d+', raw)\n",
    "    if match:\n",
    "        v = int(match.group())\n",
    "        return v if v >= 0 else 0\n",
    "    return 0\n",
    "\n",
    "def extract_candidate_list(text: str):\n",
    "    try:\n",
    "        start = text.find(\"[\")\n",
    "        end = text.rfind(\"]\") + 1\n",
    "        if start == -1 or end <= start:\n",
    "            return []\n",
    "        return ast.literal_eval(text[start:end])\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "with open(INFER_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    infer = json.load(f)\n",
    "\n",
    "per_problem = []\n",
    "by_count = {}\n",
    "total_correct = 0\n",
    "total_wrong = 0\n",
    "total_ref = 0\n",
    "total_samples = 0\n",
    "\n",
    "for rec in infer:\n",
    "    idx = rec[\"id\"]\n",
    "    item = dataset[\"train\"][idx]\n",
    "    current_count = int(item[\"count\"])\n",
    "    correct = item[\"sentences\"]\n",
    "    ref_len = len(correct)\n",
    "\n",
    "    generated_texts = [rec[\"generated_responses\"][0]]\n",
    "    n_samples = len(generated_texts)\n",
    "\n",
    "    correct_sum = 0\n",
    "    wrong_sum = 0\n",
    "\n",
    "    for text in generated_texts:\n",
    "        candidate_list = extract_candidate_list(text)\n",
    "        score = llm_judge(candidate_list, correct, mode=MODE, model=JUDGE_MODEL)\n",
    "        if score > ref_len:\n",
    "            score = ref_len\n",
    "        correct_sum += score\n",
    "        wrong_sum += (ref_len - score)\n",
    "\n",
    "    per_problem.append({\n",
    "        \"id\": int(idx),\n",
    "        \"count\": current_count,\n",
    "        \"n_samples\": int(n_samples),\n",
    "        \"reference_len\": int(ref_len),\n",
    "        \"correct_sum\": int(correct_sum),\n",
    "        \"wrong_sum\": int(wrong_sum),\n",
    "    })\n",
    "\n",
    "    if current_count not in by_count:\n",
    "        by_count[current_count] = {\"correct_sum\": 0, \"wrong_sum\": 0, \"reference_len_sum\": 0, \"n_samples_sum\": 0, \"n_problems\": 0}\n",
    "    by_count[current_count][\"correct_sum\"] += int(correct_sum)\n",
    "    by_count[current_count][\"wrong_sum\"] += int(wrong_sum)\n",
    "    by_count[current_count][\"reference_len_sum\"] += int(ref_len) * int(n_samples)\n",
    "    by_count[current_count][\"n_samples_sum\"] += int(n_samples)\n",
    "    by_count[current_count][\"n_problems\"] += 1\n",
    "\n",
    "    total_correct += int(correct_sum)\n",
    "    total_wrong += int(wrong_sum)\n",
    "    total_ref += int(ref_len) * int(n_samples)\n",
    "    total_samples += int(n_samples)\n",
    "\n",
    "out = {\n",
    "    \"mode\": MODE,\n",
    "    \"judge_model\": JUDGE_MODEL,\n",
    "    \"summary\": {\n",
    "        \"total_correct\": int(total_correct),\n",
    "        \"total_wrong\": int(total_wrong),\n",
    "        \"total_reference\": int(total_ref),\n",
    "        \"total_samples\": int(total_samples),\n",
    "        \"accuracy_like\": float(total_correct / total_ref) if total_ref > 0 else 0.0,\n",
    "    },\n",
    "    \"by_count\": {str(k): v for k, v in sorted(by_count.items(), key=lambda x: x[0])},\n",
    "    \"per_problem\": per_problem,\n",
    "}\n",
    "\n",
    "with open(\"comparison_results_all.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "x_axis = sorted(by_count.keys())\n",
    "y_axis = []\n",
    "for k in x_axis:\n",
    "    denom = by_count[k][\"reference_len_sum\"]\n",
    "    y_axis.append(by_count[k][\"correct_sum\"] / denom if denom > 0 else 0.0)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_axis, y_axis, marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Count (All Samples)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"accuracy_by_count_all.png\")\n",
    "\n",
    "print(\"Finished processing 350 samples.\")\n",
    "print(FileLink(\"comparison_results_all.json\"))\n",
    "print(FileLink(\"accuracy_by_count_all.png\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
